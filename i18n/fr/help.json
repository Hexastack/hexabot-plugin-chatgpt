{
  "token": "Entrez votre jeton d'API pour authentifier les requêtes vers le service ChatGPT.",
  "model": "Sélectionnez la version du modèle à utiliser pour générer les réponses (par exemple, gpt-3.5-turbo).",
  "context": "Fournissez un contexte initial ou des informations que le modèle devrait prendre en compte lors de la génération des réponses.",
  "instructions": "Spécifiez des instructions spéciales ou des directives que le modèle devrait suivre dans ses réponses.",
  "num_ctx": "Définissez le nombre maximum de jetons (mots et ponctuations) que le modèle peut considérer à partir du contexte fourni.",
  "max_messages_ctx": "Déterminez le nombre maximum de messages d'interactions précédentes que le modèle doit se souvenir et considérer lorsqu'il répond.",
  "frequency_penalty": "Pénalise les nouveaux tokens en fonction de leur fréquence dans le texte généré. Une valeur positive réduit la probabilité de répéter le même token. (Par défaut : 0, plage : -2.0 à 2.0)",
  "function_call": "Détermine si le modèle doit appeler une fonction ou générer un message. 'none' désactive les appels de fonction, 'auto' permet au modèle de choisir, et spécifier une fonction force le modèle à l'appeler. (Par défaut : 'none')",
  "logit_bias": "Modifie la probabilité d'apparition des tokens spécifiés. Accepte un objet JSON mappant des tokens à une valeur de biais (-100 à 100). Un biais fortement positif rend un token plus probable, tandis qu'une valeur négative réduit la probabilité.",
  "logprobs": "Si activé, renvoie les probabilités logarithmiques de chaque token de sortie, utile pour comprendre la sélection des tokens. (Par défaut : false)",
  "max_completion_tokens": "Limite le nombre maximal de tokens dans la complétion, aidant à contrôler les coûts et à éviter les sorties trop longues. (Par défaut : 1000)",
  "n": "Contrôle combien de complétions générer pour chaque invite. (Par défaut : 1)",
  "parallel_tool_calls": "Active ou désactive les appels de fonction parallèles lors de l'utilisation d'outils. (Par défaut : false)",
  "presence_penalty": "Pénalise les nouveaux tokens en fonction de leur apparition dans la conversation, encourageant le modèle à aborder de nouveaux sujets. (Par défaut : 0, plage : -2.0 à 2.0)",
  "response_format": "Spécifie le format de la réponse du modèle. Les options incluent 'text' pour une sortie classique et 'json' pour des réponses structurées. (Par défaut : 'text')",
  "seed": "Spécifie une graine aléatoire pour des réponses déterministes. Utiliser la même graine avec les mêmes paramètres renverra le même résultat. (Par défaut : null)",
  "stop": "Spécifie des séquences d'arrêt qui empêchent la génération de tokens supplémentaires lorsqu'elles sont rencontrées. Vous pouvez spécifier une ou plusieurs séquences d'arrêt. (Par défaut : null)",
  "store": "Indique si la sortie de la requête doit être stockée pour une utilisation future dans la distillation ou l'évaluation du modèle. (Par défaut : false)",
  "stream": "Si activé, renvoie des deltas partiels de messages au fur et à mesure que la complétion est générée, similaire au streaming de ChatGPT. (Par défaut : false)",
  "temperature": "Contrôle l'imprévisibilité de la sortie. Des valeurs plus élevées rendent la sortie plus créative, tandis que des valeurs plus faibles la rendent plus focalisée et déterministe. (Par défaut : 0.8, plage : 0 à 2.0)",
  "tool_choice": "Contrôle quel outil (le cas échéant) le modèle doit utiliser. 'none' désactive l'utilisation des outils, 'auto' laisse le modèle choisir, et 'required' force l'utilisation de l'outil. (Par défaut : 'auto')",
  "top_logprobs": "Spécifie le nombre de tokens les plus probables à renvoyer avec leurs probabilités logarithmiques associées. Nécessite que 'logprobs' soit activé. (Par défaut : null)",
  "top_p": "Alternative à la température, utilisant l'échantillonnage nucleus pour considérer les tokens avec une masse de probabilité combinée. Une valeur plus faible (ex : 0.5) génère un texte plus focalisé, tandis qu'une valeur plus élevée (ex : 0.95) génère un texte plus diversifié. (Par défaut : 0.9)",
  "user": "Un identifiant unique représentant votre utilisateur final, pouvant être utilisé pour la surveillance et la détection des abus. (Par défaut : chaîne vide)"
}
