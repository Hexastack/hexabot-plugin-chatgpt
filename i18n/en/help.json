{
  "token": "Enter your API token to authenticate requests to the ChatGPT service.",
  "model": "Select the model version to use for generating responses (e.g., gpt-3.5-turbo).",
  "context": "Provide initial context or information that the model should consider when generating responses.",
  "instructions": "Specify any special instructions or guidelines that the model should follow in its responses.",
  "num_ctx": "Set the maximum number of tokens (words and punctuation) that the model can consider from the provided context.",
  "max_messages_ctx": "Define the maximum number of previous interaction messages that the model should remember and consider when responding.",
  "frequency_penalty": "Penalizes new tokens based on their frequency in the generated text. A positive value reduces the likelihood of repeating the same token. (Default: 0, range: -2.0 to 2.0)",
  "function_call": "Determines if the model should call a function or generate a message. 'none' disables function calls, 'auto' allows the model to choose, and specifying a function forces the model to call it. (Default: 'none')",
  "logit_bias": "Modifies the likelihood of specified tokens appearing. Accepts a JSON object mapping tokens to a bias value (-100 to 100). A high positive bias makes a token more likely, while a negative value reduces the likelihood.",
  "logprobs": "If true, returns the log probabilities of each output token, useful for understanding token selection. (Default: false)",
  "max_completion_tokens": "Limits the maximum number of tokens in the completion, helping to control costs and avoid long outputs. (Default: 1000)",
  "n": "Controls how many completions to generate for each prompt. (Default: 1)",
  "parallel_tool_calls": "Enables or disables parallel function calls when using tools. (Default: false)",
  "presence_penalty": "Penalizes new tokens based on their appearance in the conversation, encouraging the model to talk about new topics. (Default: 0, range: -2.0 to 2.0)",
  "response_format": "Specifies the format of the model's output. Options include 'text' for regular output and 'json' for structured responses. (Default: 'text')",
  "seed": "Specifies a random seed for deterministic responses. Using the same seed with the same parameters will return the same result. (Default: null)",
  "stop": "Specifies stop sequences that prevent further token generation when encountered. You can specify one or more stop sequences. (Default: null)",
  "store": "Indicates whether to store the output of the request for future use in model distillation or evaluation. (Default: false)",
  "stream": "If enabled, returns partial message deltas as the completion is generated, similar to ChatGPT streaming. (Default: false)",
  "temperature": "Controls the randomness of the output. Higher values make the output more creative, while lower values make it more focused and deterministic. (Default: 0.8, range: 0 to 2.0)",
  "tool_choice": "Controls which tool (if any) the model should call. 'none' disables tool use, 'auto' lets the model choose, and 'required' forces tool usage. (Default: 'auto')",
  "top_logprobs": "Specifies the number of most likely tokens to return with their associated log probabilities. Requires 'logprobs' to be true. (Default: null)",
  "top_p": "Alternative to temperature, using nucleus sampling to consider tokens with a combined probability mass. A lower value (e.g., 0.5) generates more focused text, while a higher value (e.g., 0.95) generates more diverse text. (Default: 0.9)",
  "user": "A unique identifier representing your end-user, which can be used for monitoring and detecting abuse. (Default: empty string)"
}
